{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first link to extract\n",
    "\n",
    "if number_ep_database == 0:\n",
    "    link  = url\n",
    "else:\n",
    "    # Extract last link in database\n",
    "    try:\n",
    "            # Execute a query\n",
    "        cursor.execute(f\"SELECT link from {table_name} order by Episod desc limit 1\")\n",
    "        link = cursor.fetchall()\n",
    "        link = str(link[0][0])\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "           \n",
    "    soup = create_soup(link)\n",
    "    link = soup.find_all('a', class_='ipc-icon-button sc-3f4e3993-3 iasCTO ipc-icon-button--baseAlt ipc-icon-button--onBase')[1].attrs['href']\n",
    "    link = extract_link(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists\n",
    "season_list = []\n",
    "episod_season_list = []\n",
    "episod_list= []\n",
    "title_list = []\n",
    "rating_list = []\n",
    "day_list= []\n",
    "month_list= []\n",
    "year_list = []\n",
    "link_list = []\n",
    "\n",
    "link_list.append(link)\n",
    "\n",
    "\n",
    "for i in range(batch_ep_to_extract):\n",
    "\n",
    "    try:\n",
    "\n",
    "        write_to_file(log_file, f\"Extracting episod {number_ep_database+1+i} from {link}\")\n",
    "        soup = create_soup(link)\n",
    "        \n",
    "        # Season and Episod\n",
    "        ep = soup.find('div', class_=\"sc-3f4e3993-0 fYpskP\").text\n",
    "        season,episod = extract_episod(ep)\n",
    "        episod_list.append(episod)\n",
    "        \n",
    "        # Title\n",
    "        title = extract_title(soup.find('span', class_=\"hero__primary-text\").text)\n",
    "        \n",
    "        title_list.append(title)\n",
    "\n",
    "        #Rating\n",
    "        rate = soup.find('span', class_=\"sc-c4ffe080-1 iQZtLP\").text.replace(\",\",\".\")\n",
    "        rating_list.append(float(rate))\n",
    "\n",
    "        # Date\n",
    "        date = soup.find_all('li', class_=\"ipc-inline-list__item\")\n",
    "        day,month,year = extract_date(date)\n",
    "        day_list.append(day)\n",
    "        month_list.append(month)\n",
    "        year_list.append(year)\n",
    "\n",
    "        #Link \n",
    "        if i == 0 and number_ep_database == 0: \n",
    "            link = soup.find('a', class_='ipc-icon-button sc-3f4e3993-3 iasCTO ipc-icon-button--baseAlt ipc-icon-button--onBase').attrs['href']\n",
    "        else:\n",
    "            link = soup.find_all('a', class_='ipc-icon-button sc-3f4e3993-3 iasCTO ipc-icon-button--baseAlt ipc-icon-button--onBase')[1].attrs['href']\n",
    "\n",
    "        if i < batch_ep_to_extract - 1:        \n",
    "            link = extract_link(link)\n",
    "            link_list.append(link)\n",
    "\n",
    "        i = i+1\n",
    "\n",
    "    except IndexError:\n",
    "        pass         \n",
    "\n",
    "    except WebDriverException as E: \n",
    "        write_to_file(log_file, f\"Extracting episod {number_ep_database+1+i} from {link} failed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da997ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the season are wrong in IMDB website, Extract season from wikipedia page\n",
    "\n",
    "# Import Requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/Lists_of_One_Piece_episodes\"\n",
    "listofseason = []\n",
    "nb_ep_season = []\n",
    "\n",
    "# Download the contents of the webpage in text format\n",
    "data  = requests.get(wiki_url).text \n",
    "\n",
    "# Create a Beautiful soup of the data\n",
    "soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "\n",
    "# Extract the tableau\n",
    "table = soup.find_all(\"table\", {\"class\": \"wikitable plainrowheaders\"})[0]\n",
    "\n",
    "# Extract the list of seasons\n",
    "links = table.find_all('a')\n",
    "for link in links:\n",
    "    listofseason.append(int(link.text)) \n",
    "\n",
    "# Extract the list of episods per season\n",
    "trs = table.find_all('tr')\n",
    "for i in trs[2:]:\n",
    "    tds = i.find_all('td')\n",
    "    nb_ep_season.append(int(tds[1].text))\n",
    "\n",
    "for i in range(1,(len(listofseason)+1)):\n",
    "    for j in range(1,(nb_ep_season[i-1]+1)):\n",
    "        episod_season_list.append(j)\n",
    "        season_list.append(i)\n",
    "\n",
    "episod_season_list =  episod_season_list[number_ep_database:(number_ep_database+batch_ep_to_extract)]\n",
    "season_list =  season_list[number_ep_database:(number_ep_database+batch_ep_to_extract)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb55a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(link_list) > len(episod_list):\n",
    "    del link_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"Episod\":episod_list,\"Season\":season_list,\"Episod_season\":episod_season_list, \"Title\":title_list, \"Day\":day_list, \"Month\":month_list, \"Year\":year_list, \"Rate\":rating_list, \"link\":link_list}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(csv_file,index=False,header=False, mode= \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d0b591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.shape[0] == batch_ep_to_extract:\n",
    "    write_to_file(log_file,f\"Extracting {batch_ep_to_extract} episod successfuly\")\n",
    "else:\n",
    "    write_to_file(log_file,f\"Extraction failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cef98acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(log_file,f\"Sending {len(df)} rows to MySQL table '{table_name}'.\")\n",
    "try:\n",
    "    # Create a list of column names from the DataFrame\n",
    "    columns = ', '.join(df.columns)\n",
    "\n",
    "    # Create a placeholders string for the SQL query\n",
    "    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "\n",
    "    # Create the SQL query\n",
    "    sql = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "    # Execute the SQL query with the DataFrame values\n",
    "    cursor.executemany(sql, df.values.tolist())\n",
    "\n",
    "    # Commit the changes to the database\n",
    "    cnx.commit()\n",
    "\n",
    "    write_to_file(log_file,f\"Successfully sent {len(df)} rows to MySQL table '{table_name}'.\")\n",
    "except mysql.connector.Error as e:\n",
    "    print(f\"Error sending data to MySQL: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
