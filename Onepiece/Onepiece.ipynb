{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d74e10",
   "metadata": {},
   "source": [
    "\n",
    "   # One piece Extraction from IMDB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6408c30",
   "metadata": {},
   "source": [
    "#### Install Library necessary for the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d96955-d62f-4614-965a-4286124bf011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yacbo\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yacbo\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bs4 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mysql-connector-python in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (9.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: selenium in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\yacbo\\appdata\\roaming\\python\\python311\\site-packages (from selenium) (4.12.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (3.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yacbo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Pandas\n",
    "%pip install pandas\n",
    "\n",
    "# Python Dotenv\n",
    "%pip install python-dotenv\n",
    "\n",
    "# Beautiful soup\n",
    "%pip install bs4\n",
    "\n",
    "# MySQL Connector\n",
    "%pip install mysql-connector-python\n",
    "\n",
    "# Selenium\n",
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7efa7a",
   "metadata": {},
   "source": [
    "#### Import Library necessary for the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9acdfd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MySQL connector\n",
    "import mysql.connector\n",
    "\n",
    "# Web driver \n",
    "from selenium import webdriver\n",
    "\n",
    "# Beautiful soup\n",
    "from bs4 import BeautifulSoup # this module helps in web scrapping.\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# importing os module for environment variables\n",
    "import os\n",
    "\n",
    "# time\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# importing necessary functions from dotenv library\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# loading variables from .env file\n",
    "load_dotenv() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced147ba",
   "metadata": {},
   "source": [
    "#### Create the necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8103e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(filename, content, mode='a'):\n",
    "    \"\"\"Writes the given log to the log file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the file to add the content.\n",
    "        content (str): The content to write to the file.\n",
    "        mode (str, optional): The file opening mode. Defaults to 'a' (append).\n",
    "            Other modes include:\n",
    "            - 'w' (write)\n",
    "            - 'r+' (read and write)\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(filename, mode, encoding='utf-8') as file:\n",
    "            file.write(str(datetime.now()) + \": \" + content + \"\\n\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to content to log file.': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b900be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(url,sleep = 1):\n",
    "    \"\"\"Extract data from the url.\n",
    "\n",
    "    Args:\n",
    "        url (str): The link to the data to be extracted.\n",
    "        sleep (int, optional): The time to wait before extracted the page web. Defaults to 1s.\n",
    "    \"\"\"\n",
    "        \n",
    "    driver = webdriver.Firefox() \n",
    "    # Webscrape the URL\n",
    "    driver.get(url)\n",
    "    # wait 5s\n",
    "    time.sleep(sleep)\n",
    "    # Create a Beautiful soup object\n",
    "    soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    # close driver\n",
    "    driver.close()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f81e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number_ep():\n",
    "    \"\"\"Extract number of data from the beautiful soup.\n",
    "    \"\"\"\n",
    "    soup = create_soup(\"https://www.imdb.com/title/tt0388629/episodes/?ref_=tt_ep_epl\",5)\n",
    "    number_ep = soup.find(\"li\", class_ = \"ipc-inline-list__item sc-a68beae9-0 jefvJo no-text-overflow\").text\n",
    "    number_ep = int(number_ep.split(\".E\")[1])\n",
    "    return number_ep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf45660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(date):\n",
    "    \"\"\"Extract date from the beautiful soup..\n",
    "\n",
    "    Args:\n",
    "        date (str): String including the date extracted from beautiful soup.\n",
    "    \"\"\"\n",
    "    for i in range(10):\n",
    "        if \"Épisode\" in date[i].text:\n",
    "            date = date[i].text\n",
    "            date_list=[]\n",
    "            date_list = date[date.find(\"le\")+3:].split()\n",
    "            day = int(date_list[0])\n",
    "            month = int(date_list[1].replace(\"janv.\",\"01\").replace(\"févr.\",\"02\").replace(\"mars\",\"03\").replace(\"avr.\",\"04\").replace(\"mai\",\"05\").replace(\"juin\",\"06\").replace(\"juil.\",\"07\").replace(\"août\",\"08\").replace(\"sept.\",\"09\").replace(\"oct.\",\"10\").replace(\"nov.\",\"11\").replace(\"déc.\",\"12\"))\n",
    "            year = int(date_list[2])\n",
    "            return day,month,year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c405bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_episod(ep):\n",
    "    \"\"\"Extract episod from the beautiful soup..\n",
    "\n",
    "    Args:\n",
    "        ep (str): String including the episod extracted from beautiful soup.\n",
    "    \"\"\"\n",
    "    ep_list=[]\n",
    "    ep_list = ep.split(\".E\")\n",
    "    episod = int(ep_list[1])\n",
    "    return episod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6524f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link(link):\n",
    "    \"\"\"Extract link from the beautiful soup..\n",
    "\n",
    "    Args:\n",
    "        link (str): String including the link extracted from beautiful soup.\n",
    "    \"\"\"\n",
    "    link = \"https://www.imdb.com/\" + link[0:link.find(\"/?ref\")]\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df778f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(title):\n",
    "    \"\"\"Extract title from the beautiful soup..\n",
    "\n",
    "    Args:\n",
    "        title (str): String including the title extracted from beautiful soup.\n",
    "    \"\"\"\n",
    "    if \",\" in title:\n",
    "        title = title.replace(\",\",\"\")\n",
    "    return title                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66272263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list(*args):\n",
    "    \"\"\"Create list if it doesn't exist\n",
    "    \"\"\"\n",
    "    mydict = {}\n",
    "    for arg in args:\n",
    "        if arg not in globals():           \n",
    "            mydict[arg] = list()\n",
    "    globals().update(mydict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da5b65",
   "metadata": {},
   "source": [
    "#### Connect to the database, create table \"IMBD\" if it doesn't exist and extract the number of row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dd88d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "host = os.getenv(\"MYSQL_HOSTNAME\")\n",
    "user = os.getenv(\"MYSQL_USERNAME\")\n",
    "password = os.getenv(\"MYSQL_PASSWORD\")\n",
    "database = \"Onepiece\"\n",
    "table_name = \"IMDB\"\n",
    "csv_file = str(datetime.now().year) + \"_\" + str(datetime.now().month) + \"_\" + str(datetime.now().day) + \"_\" + \"onepiece.csv\"\n",
    "log_file = str(datetime.now().year) + \"_\" + str(datetime.now().month) + \"_\" + str(datetime.now().day) + \"_\" + \"onepiecelog.txt\"\n",
    "url = os.getenv(\"ONEPIECE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d68a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(log_file, f\"Connecting to MySQL {database} database\",\"w\")\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "\n",
    "    cnx = mysql.connector.connect(\n",
    "        host=host,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        database=database,\n",
    "        port=3306\n",
    "    )\n",
    "\n",
    "    write_to_file(log_file, f\"Connected to {database} database\")\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Error: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b901838",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(log_file, f\"Creating table IMDB into {database} if doesn't exist\")\n",
    "\n",
    "# Create Table IMDB into MYSQL\n",
    "try:\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    # SQL statement to create the table\n",
    "    sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS IMDB (\n",
    "        Episod INT UNIQUE NOT NULL PRIMARY KEY,\n",
    "        Title VARCHAR(255),\n",
    "        Day INT,\n",
    "        Month INT,\n",
    "        Year INT NOT NULL,\n",
    "        Rate DOUBLE,\n",
    "        Link VARCHAR(255)\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the SQL statement\n",
    "    cursor.execute(sql)\n",
    "\n",
    "    # Commit the changes to the database\n",
    "    cnx.commit()\n",
    "\n",
    "    write_to_file(log_file, f\"Table {table_name} created into {database} successfully\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Error: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0abdb348",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(log_file, f\"Extracting number total of episode and number of episod already in IMDB table {database}\")\n",
    "\n",
    "# Extract number of One piece episod\n",
    "\n",
    "number_tot_ep = extract_number_ep()\n",
    "\n",
    "write_to_file(log_file, f\"There are {number_tot_ep} Onepiece episods at this time\")\n",
    "\n",
    "# Extract number of episod in database\n",
    "try:\n",
    "         # Execute a query\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM IMDB\")\n",
    "    number_ep_database = cursor.fetchall()\n",
    "    number_ep_database = int(number_ep_database[0][0])\n",
    "\n",
    "    write_to_file(log_file, f\"There is(are) {number_ep_database} episod(s) already in IMDB table\")\n",
    "\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Error: {err}\")\n",
    "\n",
    "number_ep_to_extract = number_tot_ep - number_ep_database\n",
    "\n",
    "write_to_file(log_file, f\"There is(are) {number_ep_to_extract} episod(s) to extract\")\n",
    "\n",
    "if number_ep_to_extract == 0 :\n",
    "    write_to_file(log_file, f\"Program stopped because there is no data to extract\")\n",
    "    raise SystemExit(\"System stop because there is no episod to extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61d8e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first link to extract\n",
    "\n",
    "if number_ep_database == 0:\n",
    "    link  = \"https://www.imdb.com/title/tt0947442/?ref_=ttep_ep1\"\n",
    "else:\n",
    "    # Extract last link in database\n",
    "    try:\n",
    "            # Execute a query\n",
    "        cursor.execute(\"SELECT link from imdb order by Episod desc limit 1\")\n",
    "        link = cursor.fetchall()\n",
    "        link = str(link[0][0])\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "           \n",
    "    soup = create_soup(link)\n",
    "    link = soup.find_all('a', class_='ipc-icon-button sc-3f4e3993-3 iasCTO ipc-icon-button--baseAlt ipc-icon-button--onBase')[1].attrs['href']\n",
    "    link = extract_link(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "047b47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_list(\"episod_list\",\"title_list\",\"rating_list\",\"day_list\",\"month_list\",\"year_list\",\"link_list\")\n",
    "\n",
    "link_list.append(link)\n",
    "\n",
    "try:\n",
    "\n",
    "    for i in range(number_ep_to_extract):\n",
    "\n",
    "        write_to_file(log_file, f\"Extracting episod {number_ep_database+1+i} from {link}\")\n",
    "        soup = create_soup(link)\n",
    "        \n",
    "        # Season and Episod\n",
    "        ep = soup.find('div', class_=\"sc-3f4e3993-0 fYpskP\").text\n",
    "        episod = extract_episod(ep)\n",
    "        episod_list.append(episod)\n",
    "\n",
    "        # Title\n",
    "        title = soup.find('span', class_=\"hero__primary-text\").text\n",
    "        title_list.append(title)\n",
    "\n",
    "        #Rating\n",
    "        rate = soup.find('span', class_=\"sc-c4ffe080-1 iQZtLP\").text.replace(\",\",\".\")\n",
    "        rating_list.append(float(rate))\n",
    "\n",
    "        # Date\n",
    "        date = soup.find_all('li', class_=\"ipc-inline-list__item\")\n",
    "        day,month,year = extract_date(date)\n",
    "        day_list.append(day)\n",
    "        month_list.append(month)\n",
    "        year_list.append(year)\n",
    "\n",
    "        #Link \n",
    "        if i == 0 and number_ep_database == 0: \n",
    "            link = soup.find('a', class_='ipc-icon-button sc-3f4e3993-3 iasCTO ipc-icon-button--baseAlt ipc-icon-button--onBase').attrs['href']\n",
    "        else:\n",
    "            link = soup.find_all('a', class_='ipc-icon-button sc-3f4e3993-3 iasCTO ipc-icon-button--baseAlt ipc-icon-button--onBase')[1].attrs['href']\n",
    "\n",
    "        if i < number_ep_to_extract - 1:        \n",
    "            link = extract_link(link)\n",
    "            link_list.append(link)\n",
    "\n",
    "        i = i+1\n",
    "\n",
    "except WebDriverException as E: \n",
    "    write_to_file(log_file, f\"Extracting episod {number_ep_database+1+i} from {link} failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb55a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(link_list) > len(episod_list):\n",
    "    del link_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19e3085e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episod</th>\n",
       "      <th>Title</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rate</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1120</td>\n",
       "      <td>The World Is Shaken! The Ruler's Judgment and ...</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>2024</td>\n",
       "      <td>8.6</td>\n",
       "      <td>https://www.imdb.com//title/tt33394990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Episod                                              Title  Day  Month  \\\n",
       "0    1120  The World Is Shaken! The Ruler's Judgment and ...   22      9   \n",
       "\n",
       "   Year  Rate                                    link  \n",
       "0  2024   8.6  https://www.imdb.com//title/tt33394990  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onepiece_data = {\"Episod\":episod_list, \"Title\":title_list, \"Day\":day_list, \"Month\":month_list, \"Year\":year_list, \"Rate\":rating_list, \"link\":link_list}\n",
    "onepiece_df = pd.DataFrame(onepiece_data)\n",
    "onepiece_df.to_csv(csv_file,index=False,mode= \"a\",header=False)\n",
    "onepiece_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d0b591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if onepiece_df.shape[0] == number_ep_to_extract:\n",
    "    write_to_file(log_file,f\"Extracting {number_ep_to_extract} episod successfuly\")\n",
    "else:\n",
    "    write_to_file(log_file,f\"Extraction failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cef98acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(log_file,f\"Sending {len(onepiece_df)} rows to MySQL table '{table_name}'.\")\n",
    "try:\n",
    "    # Create a list of column names from the DataFrame\n",
    "    columns = ', '.join(onepiece_df.columns)\n",
    "\n",
    "    # Create a placeholders string for the SQL query\n",
    "    placeholders = ', '.join(['%s'] * len(onepiece_df.columns))\n",
    "\n",
    "    # Create the SQL query\n",
    "    sql = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "    # Execute the SQL query with the DataFrame values\n",
    "    cursor.executemany(sql, onepiece_df.values.tolist())\n",
    "\n",
    "    # Commit the changes to the database\n",
    "    cnx.commit()\n",
    "\n",
    "    write_to_file(log_file,f\"Successfully sent {len(onepiece_df)} rows to MySQL table '{table_name}'.\")\n",
    "except mysql.connector.Error as e:\n",
    "    print(f\"Error sending data to MySQL: {e}\")\n",
    "finally:\n",
    "    # Close the database cursor and connection\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if cnx:\n",
    "        cnx.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
